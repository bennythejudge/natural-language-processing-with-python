{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# esercizio: trova un documento esempio un articolo di giornale o forse un intero libro\n",
    "# prendi una query sentence\n",
    "# costruisci il dictionary\n",
    "# trova le frasi most similar to the query sentence\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['They', 'thought', 'they', \"'d\", 'get', 'there', 'sooner'], ['We', 'believe', 'this', 'is', 'justice'], ['Somehow', 'are', 'values', 'have', 'been', 'corrupted'], ['Something', 'stinks', 'here'], ['Some', 'other', 'random', 'sentence', 'should', 'appear', 'here'], ['Love', 'is', 'a', 'four', 'letter', 'word'], ['The', 'situations', 'confirm', 'their', 'worries'], ['we', 'love', 'fried', 'chicken'], ['we', 'hate', 'fried', 'chicken'], ['we', 'love', 'icecream'], ['we', 'hate', 'icecream'], ['I', 'love', 'icecream'], ['Mr.', 'Darcy', 'comes', 'from', 'the', 'city'], ['Italians', 'eat', 'pizza']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# now playing with tf-idf and more exoteric stuff\n",
    "import gensim\n",
    "\n",
    "# load pride and prejudice and tokenize it using nltk\n",
    "#f = open(\"/Users/neo/Desktop/workspace/natural-language-text-processing-with-python/pap.txt\", 'r')\n",
    "# lines = f.readlines()\n",
    "# data = ''\n",
    "# I am not 100% sure about the following - TODO: check!\n",
    "# for line in lines:\n",
    "#     data = data + \" \" + line.strip()\n",
    "\n",
    "data = [\"They thought they'd get there sooner\",\n",
    "        \"We believe this is justice\",\n",
    "        \"Somehow are values have been corrupted\",\n",
    "        \"Something stinks here\",\n",
    "       \"Some other random sentence should appear here\",\n",
    "       \"Love is a four letter word\",\n",
    "       \"The situations confirm their worries\",\n",
    "       \"we love fried chicken\",\n",
    "       \"we hate fried chicken\",\n",
    "       \"we love icecream\",\n",
    "       \"we hate icecream\",\n",
    "       \"I love icecream\",\n",
    "       \"Mr. Darcy comes from the city\",\n",
    "       \"Italians eat pizza\"]\n",
    "\n",
    "    \n",
    "type(data)\n",
    "# some metrics\n",
    "len(data)\n",
    "\n",
    "# # nltk tokenize it\n",
    "gen_docs = []\n",
    "for sentence in data:\n",
    "    gen_docs.append(word_tokenize(sentence))\n",
    "# tokens_nltk = word_tokenize(data)\n",
    "# print(tokens_nltk)\n",
    "print(gen_docs)\n",
    "\n",
    "# # let's get rid of stop words\n",
    "# english_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "# #print(english_stopwords)\n",
    "# #print(len(english_stopwords))\n",
    "# stop_words_free_tokens_nltk = [token for token in tokens_nltk if token.lower() not in english_stopwords]\n",
    "# print(\"initial token count: {} - post-stop-words cleanup token count: {}\".format(len(tokens_nltk),len(stop_words_free_tokens_nltk)))\n",
    "# #print(stop_words_free_tokens_nltk)\n",
    "# print(type(stop_words_free_tokens_nltk))\n",
    "# # I have been having a problem here with gen_docs - the gensim.corpora.Dictionary would not\n",
    "# # accept my list of tokens with error: \n",
    "# # doc2bow expects an array of unicode tokens on input, not a single string\n",
    "# # in the end I found what looks the solution: make an array of the array of tokens\n",
    "# # it looks like the Dictionary method expects an array of array(s)\n",
    "# gen_docs = [ stop_words_free_tokens_nltk ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words in dictionary: 53\n",
      "0 They\n",
      "1 thought\n",
      "2 they\n",
      "3 'd\n",
      "4 get\n",
      "5 there\n",
      "6 sooner\n",
      "7 We\n",
      "8 believe\n",
      "9 this\n",
      "10 is\n",
      "11 justice\n",
      "12 Somehow\n",
      "13 are\n",
      "14 values\n",
      "15 have\n",
      "16 been\n",
      "17 corrupted\n",
      "18 Something\n",
      "19 stinks\n",
      "20 here\n",
      "21 Some\n",
      "22 other\n",
      "23 random\n",
      "24 sentence\n",
      "25 should\n",
      "26 appear\n",
      "27 Love\n",
      "28 a\n",
      "29 four\n",
      "30 letter\n",
      "31 word\n",
      "32 The\n",
      "33 situations\n",
      "34 confirm\n",
      "35 their\n",
      "36 worries\n",
      "37 we\n",
      "38 love\n",
      "39 fried\n",
      "40 chicken\n",
      "41 hate\n",
      "42 icecream\n",
      "43 I\n",
      "44 Mr.\n",
      "45 Darcy\n",
      "46 comes\n",
      "47 from\n",
      "48 the\n",
      "49 city\n",
      "50 Italians\n",
      "51 eat\n",
      "52 pizza\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "num_words = len(dictionary)\n",
    "print(\"Num words in dictionary: {}\".format(num_words))\n",
    "for idx,word in dictionary.items():\n",
    "    print(idx,word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get\n",
      "get\n",
      "34\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[4])\n",
    "print(dictionary.id2token[4])\n",
    "print(dictionary.token2id['confirm'])\n",
    "print(dictionary.token2id['love'])\n",
    "love_id = dictionary.token2id['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a bag of words\n",
    "# bag of words is a tf-idf term\n",
    "bow_doc = dictionary.doc2bow(stop_words_free_tokens_nltk)\n",
    "#print(bow_doc)\n",
    "# love is nr 120\n",
    "# this should be the frequency of the noun love\n",
    "# print(bow_doc[love_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(7, 1), (8, 1), (9, 1), (10, 1), (11, 1)], [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(18, 1), (19, 1), (20, 1)], [(20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)], [(10, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)], [(32, 1), (33, 1), (34, 1), (35, 1), (36, 1)], [(37, 1), (38, 1), (39, 1), (40, 1)], [(37, 1), (39, 1), (40, 1), (41, 1)], [(37, 1), (38, 1), (42, 1)], [(37, 1), (41, 1), (42, 1)], [(38, 1), (42, 1), (43, 1)], [(44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1)], [(50, 1), (51, 1), (52, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# create corpus - a list of bag of words\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=14, num_nnz=65)\n"
     ]
    }
   ],
   "source": [
    "# create the tf-idf model from the corpus\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['They', 'thought', 'they', \"'d\", 'get', 'there', 'sooner'], ['We', 'believe', 'this', 'is', 'justice'], ['Somehow', 'are', 'values', 'have', 'been', 'corrupted'], ['Something', 'stinks', 'here'], ['Some', 'other', 'random', 'sentence', 'should', 'appear', 'here'], ['Love', 'is', 'a', 'four', 'letter', 'word'], ['The', 'situations', 'confirm', 'their', 'worries'], ['we', 'love', 'fried', 'chicken'], ['we', 'hate', 'fried', 'chicken'], ['we', 'love', 'icecream'], ['we', 'hate', 'icecream'], ['I', 'love', 'icecream'], ['Mr.', 'Darcy', 'comes', 'from', 'the', 'city'], ['Italians', 'eat', 'pizza']]\n",
      "[(0, 0.3779644730092272), (1, 0.3779644730092272), (2, 0.3779644730092272), (3, 0.3779644730092272), (4, 0.3779644730092272), (5, 0.3779644730092272), (6, 0.3779644730092272)]\n"
     ]
    }
   ],
   "source": [
    "# Show document in text form, bag of words, and tf-idf\n",
    "# 0 is tacos, 1 is love, 2 is I\n",
    "# Value for I is lower because occurs multiple times.\n",
    "# Value for '.' is 0 because it occurs in all sentences and log_2(1) = 0.\n",
    "# Vectors are normalized so they sum to 1\n",
    "print(gen_docs)\n",
    "# print(gen_docs[0][love_id])\n",
    "# print(corpus[love_id])\n",
    "# print(corpus[0][love_id])\n",
    "# print(tf_idf[corpus][0])\n",
    "print(tf_idf[corpus][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: il modo in cui hai generato gen_docs non e' corretto\n",
    "# devi fare break up del document cosi' che ogni sentence sia una list mentre ora ogni parola e' una lista\n",
    "# torna indietro e fix it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 They\n",
      "1 thought\n",
      "2 they\n",
      "3 'd\n",
      "4 get\n",
      "5 there\n",
      "6 sooner\n",
      "7 We\n",
      "8 believe\n",
      "9 this\n",
      "10 is\n",
      "11 justice\n",
      "12 Somehow\n",
      "13 are\n",
      "14 values\n",
      "15 have\n",
      "16 been\n",
      "17 corrupted\n",
      "18 Something\n",
      "19 stinks\n",
      "20 here\n",
      "21 Some\n",
      "22 other\n",
      "23 random\n",
      "24 sentence\n",
      "25 should\n",
      "26 appear\n",
      "27 Love\n",
      "28 a\n",
      "29 four\n",
      "30 letter\n",
      "31 word\n",
      "32 The\n",
      "33 situations\n",
      "34 confirm\n",
      "35 their\n",
      "36 worries\n",
      "37 we\n",
      "38 love\n",
      "39 fried\n",
      "40 chicken\n",
      "41 hate\n",
      "42 icecream\n",
      "43 I\n",
      "44 Mr.\n",
      "45 Darcy\n",
      "46 comes\n",
      "47 from\n",
      "48 the\n",
      "49 city\n",
      "50 Italians\n",
      "51 eat\n",
      "52 pizza\n",
      "[(1, 1), (38, 2), (42, 1), (43, 2), (48, 1)]\n"
     ]
    }
   ],
   "source": [
    "for idx,word in dictionary.items():\n",
    "    print(idx,word)\n",
    "bow_doc = dictionary.doc2bow(['I', 'love','icecream','and','I','love','the','thought','of','it','.'])\n",
    "print(bow_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'believe', 'this', 'is', 'justice']\n",
      "[(7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
      "[(7, 0.4691328650311353), (8, 0.4691328650311353), (9, 0.4691328650311353), (10, 0.34591533616004116), (11, 0.4691328650311353)]\n"
     ]
    }
   ],
   "source": [
    "print(gen_docs[1])\n",
    "print(corpus[1])\n",
    "print(tf_idf[corpus][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(38, 1), (43, 1), (52, 1)]\n"
     ]
    }
   ],
   "source": [
    "bow = dictionary.doc2bow(['I', 'love','pizza', '.'])\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(38, 0.38152487512255495), (43, 0.6536202145216742), (52, 0.6536202145216742)]\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 14 documents in 0 shards (stored under /Users/neo/)\n"
     ]
    }
   ],
   "source": [
    "sims = gensim.similarities.Similarity('/Users/neo/', tf_idf[corpus], num_features=len(dictionary))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Italians', 'with', 'pizza', 'love']\n",
      "[(38, 1), (50, 1), (52, 1)]\n",
      "[(38, 0.38152487512255495), (50, 0.6536202145216742), (52, 0.6536202145216742)]\n"
     ]
    }
   ],
   "source": [
    "query_doc = \"Italians with pizza love\".split()\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.17319179,  0.        ,  0.23386762,\n",
       "        0.        ,  0.17174342,  0.        ,  0.75473565], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
